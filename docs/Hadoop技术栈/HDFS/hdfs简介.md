# 分布式文件系统`HDFS`

> 什么是分布式?

> 分布式是计算机程序的集合，这些程序利用跨多个独立计算节点的计算资源来实现共同的目标。分布式旨在消除系统的瓶颈或中心故障点

## 分布式文件系统

### 计算机集群结构

- 分布式文件系统把文件分布存储到多个计算机节点上，成千上万的计算机节点构成计算机集群
- 与之前使用多个处理器和专用高级硬件的并行化处理装置不同的是，目前的分布式文件系统所采用的计算机集群，都是由普通硬件构成的，这就大大降低了硬件上的开销

<center><img src='https://pic2.zhimg.com/80/v2-24da28c4540a17d6f9f3cc03a0c35e21_720w.webp'></center>

### 分布式文件系统的结构

分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类

- 一类叫“主节点”(Master Node)或者也被称为“名称结点”(NameNode)
- 另一类叫“从节点”（Slave Node）或者也被称为“数据节点”(DataNode)

<center><img src='https://pic2.zhimg.com/80/v2-785f02f9b049a9e16a26b5f45ce289d5_720w.webp'></center>

## HDFS 简介

Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的。HDFS在最开始是作为Apache Nutch搜索引擎项目的基础架构而开发的。HDFS是Apache Hadoop Core项目的一部分。这个项目的地址是https://hadoop.apache.org/core/

### HDFS的目标

- **兼容廉价的硬件设备**：在成百上千台廉价服务器中存储数据，常会出现节点失效的情况，因此 HDFS 设计了快速检测硬件故障和进行自动恢复的机制，可以实现持续监视、错误检查、容错处理和自动恢复，从而在硬件出错的情况下也能实现数据的完整性
- **流数据读写**:普通文件系统主要用于随机读写以及与用户进行交互，HDFS 则是为了满足批量数据处理的要求而设计的，因此为了提高数据吞吐率，HDFS 放松了一些 POSIX 的要求，从而能够以流式方式来访问文件系统数据。
- **大数据集**:HDFS 中的文件通常可以达到 GB 甚至 TB 级别，一个数百台机器组成的集群可以支持千万级别这样的文件
- **简单的文件模型**:HDFS 采用了「一次写入、多次读取」的简单文件模型，文件一旦完成写入，关闭后就无法再次写入，只能被读取
- **强大的跨平台兼容性**:HDFS 是采用 Java 语言实现的，具有很好的跨平台兼容性，支持 Java 虚拟机（Java Virtual Machine,JVM）的机器都可以运行 HDFS。

### HDFS的局限性

- **适合低延迟数据访问**:HDFS 主要是面向大规模数据批量处理而设计的，采用流式数据读取，具有很高的数据吞吐率，但是，这也意味着较高的延迟。因此，HDFS 不适合用在需要较低延迟（如数十毫秒）的应用场合。对于低延时要求的应用程序而言，HBase 是一个更好的选择
- **无法高效存储大量小文件**:小文件是指文件大小小于一个块的文件。HDFS 无法高效存储和处理大量小文件，过多小文件会给系统扩展性和性能带来诸多问题
- **不支持多用户写入及任意修改文件**:HDFS 只允许一个文件有一个写入者，不允许多个用户对同一个文件执行写操作，而且只允许对文件执行追加操作，不能执行随机写操作

## HDFS 的相关概念

### 块

HDFS默认一个块128MB，一个文件被分成多个块，以块作为存储单位

块的大小远远大于普通文件系统，可以**最小化寻址开销**

> HDFS 寻址开销不仅包括磁盘寻道开销，还包括数据块的定位开销。当客户端需要访问一个文件时，首先从名称节点获得组成这个文件的数据块的位置列表，然后根据位置列表获取实际存储各个数据块的数据节点的位置，最后数据节点根据数据块信息在本地 Linux 文件系统中找到对应的文件，并把数据返回给客户端。设计一个比较大的块，可以把上述寻址开销分摊到较多的数据中，降低了单位数据的寻址开销

HDFS采用抽象的块概念可以带来以下几个明显的好处：

- **支持大规模文件存储**：文件以块为单位进行存储，一个大规模文件可以被分拆成若干个文件块，不同的文件块可以被分发到不同的节点上，因此，一个文件的大小不会受到单个节点的存储容量的限制，可以远远大于网络中任意节点的存储容量
- **简化系统设计**：首先，大大简化了存储管理，因为文件块大小是固定的，这样就可以很容易计算出一个节点可以存储多少文件块；其次，方便了元数据的管理，元数据不需要和文件块一起存储，可以由其他系统负责管理元数据
- **适合数据备份**：每个文件块都可以冗余存储到多个节点上，大大提高了系统的容错性和可用性

### 名称节点

- 在HDFS中，名称节点（NameNode）负责管理分布式文件系统的命名空间，保存了两个核心的数据结构，即*FsImage*和*EditLog*
- **FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据**
    - FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据
    - FsImage文件没有记录每个块存储在哪个数据节点。而是由名称节点把这些映射信息保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。
- **操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作**
- 名称节点记录了每个文件中各个块所在的数据节点的位置信息

<center><img src='https://pic4.zhimg.com/v2-cc5e32b6012ad9961e1f5cd4208ea0fb_r.jpg'></center>

#### 名称节点的启动

- 在名称节点启动的时候，它会将FsImage文件中的内容**加载到内存中**，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作
- 一旦在内存中成功建立文件系统元数据的映射，则**创建一个新的FsImage文件和一个空的EditLog文件**
- 名称节点在启动的过程中处于「安全模式」，只能对外提供读操作，无法提供写操作。启动过程结束后，系统就会退出安全模式，进入正常运行状态，对外提供读写操作
- 名称节点起来之后，HDFS中的**更新操作会重新写到EditLog文件中**，因为FsImage文件一般都很大，如果所有的更新操作都往FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新

#### 名称节点运行期间EditLog不断变大的问题

- 在名称节点运行期间，HDFS的所有更新操作都是**直接写到EditLog中**，久而久之， EditLog文件将会变得很大
- 虽然这对名称节点运行时候是没有什么明显影响的，但是，当名称节点重启的时候，名称节点需要**先将FsImage里面的所有内容映像到内存中**，然后再一条一条地执行EditLog中的记录，当EditLog文件非常大的时候，会导致名称节点启动操作非常慢，而在这段时间内HDFS系统处于安全模式，一直无法对外提供写操作，影响了用户的使用

### 数据节点

- 数据节点是分布式文件系统HDFS的工作节点，**负责数据的存储和读取**，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表
- **每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中**

### 第二名称节点

**第二名称节点**是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是单独运行在一台机器上，具有两个方面的功能:
* **它可以完成EditLog和FsImage的合并操作，减小EditLog文件大小，缩短名称节点的重启时间**

> EditLog 与 FsImage 的合并操作。每隔一段时间，第二名称节点会和名称节点通信，请求其停止使用 EditLog 文件（这里假设这个时刻为t1），如图 3-4 所示，暂时将新到达的写操作添加到一个新的文件 EditLog.new 中。然后，第二名称节点把名称节点中的 FsImage 文件和 EditLog 文件拉回本地，再加载到内存中；对二者执行合并操作，即在内存中逐条执行 EditLog 中的操作，使 FsImage 保持最新。合并结束后，第二名称节点会把合并后得到的最新的 FsImage.ckpt 文件发送到名称节点。名称节点收到后，会用最新的 FsImage.ckpt 文件去替换旧的 FsImage 文件，同时用 EditLog.new 文件去替换 EditLog 文件（这里假设这个时刻为t2），从而减小了 EditLog 文件的大小

* **它可以作为名称节点的检查点，保存名称节点中的元数据**

> 从上面的合并过程可以看出，第二名称节点会定期和名称节点通信，从名称节点获取 FsImage 文件和 EditLog 文件，执行合并操作得到新的 FsImage.ckpt 文件。从这个角度来讲，第二名称节点相当于为名称节点设置了一个「检查点」，周期性地备份名称节点中的元数据信息，当名称节点发生故障时，就可以用第二名称节点中记录的元数据信息进行系统恢复。但是，在第二名称节点上合并操作得到的新的 FsImage 文件是合并操作发生时（即t1时刻）HDFS 记录的元数据信息，并没有包含t1时刻和t2时刻期间发生的更新操作。如果名称节点在t1时刻和t2时刻期间发生故障，系统就会丢失部分元数据信息，在 HDFS 的设计中，也并不支持把系统直接切换到第二名称节点。因此从这个角度来讲，第二名称节点只是起到了名称节点的「检查点」作用，并不能起到「热备份」作用。即使有了第二名称节点的存在，当名称节点发生故障时，系统还是有可能会丢失部分元数据信息的

<center><img src='https://cdn.jsdelivr.net/gh/weno861/image/img/202401312137030.png'></center>

## HDFS体系结构

HDFS 采用了主从（Master/Slave）结构模型，**一个 HDFS 集群包括一个名称节点和若干个数据节点**。名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问。集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地 Linux 文件系统中的。每个数据节点会周期性地向名称节点发送「心跳」信息，报告自己的状态，没有按时发送心跳信息的数据节点会被标记为「死机」，不会再给它分配任何 I/O 请求

<center><img src='https://cdn.jsdelivr.net/gh/weno861/image/img/202401312135869.png'></center>

### HDFS的命名空间

HDFS 的命名空间包含**目录、文件和块**。命名空间管理是指命名空间支持对 HDFS 中的目录、文件和块做类似文件系统的创建、修改、删除等基本操作。在当前的 HDFS 体系结构中，整个 HDFS 集群只有一个命名空间，并且只有**唯一一个名称节点**，该节点负责对这个命名空间进行管理。

### 通信协议

HDFS 是一个部署在集群上的分布式文件系统，因此很多数据需要通过网络进行传输。所有的 HDFS 通信协议都是**构建在 TCP/IP 基础之上**的。客户端通过一个可配置的端口向名称节点主动发起 TCP 连接，并使用客户端协议与名称节点进行交互。**名称节点和数据节点之间则使用数据节点协议进行交互**。客户端与数据节点的交互通过远程过程调用（Remote Procedure Call, RPC）来实现。在设计上，名称节点不会主动发起 RPC，而是响应来自客户端和数据节点的 RPC 请求

### 客户端

客户端是用户操作 HDFS 最常用的方式，HDFS 在部署时都提供了客户端。不过需要说明的是，严格来说，客户端并不算是 HDFS 的一部分。客户端可以支持打开、读取、写入等常见的操作，并且提供了类似 Shell 的命令行方式来访问 HDFS 中的数据。此外，HDFS 也提供了 Java API，**作为应用程序访问文件系统的客户端编程接口**

### HDFS体系结构的局限性

HDFS 只设置唯一一个名称节点，这样做虽然大大简化了系统设计，但也带来了一些明显的局限性，具体如下。

* **命名空间的限制**：名称节点是保存在内存中的，因此名称节点能够容纳对象（文件、块）的个数会受到内存空间大小的限制。
* **性能的瓶颈**：整个分布式文件系统的吞吐量受限于单个名称节点的吞吐量。
* **隔离问题**：由于集群中只有一个名称节点，只有一个命名空间，因此无法对不同应用程序进行隔离。
* **集群的可用性**：一旦这个唯一的名称节点发生故障，会导致整个集群变得不可用。

## HDFS的存储策略

### 数据的冗余存储

作为一个分布式文件系统，为了保证系统的容错性和可用性，HDFS采用了**多副本方式**对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上，如图所示，数据块1被分别存放到数据节点A和C上，数据块2被存放在数据节点A和B上。这种多副本方式具有以下几个优点

* **加快数据传输速度**：当多个客户端需要同时访问同一个文件时，可以让各个客户端分别从不同的数据块副本中读取数据，这就大大加快了数据传输速度。
* **容易检查数据错误**：HDFS 的数据节点之间通过网络传输数据，采用多个副本可以很容易判断数据传输是否出错
* **保证数据可靠性**：即使某个数据节点出现故障失效，也不会造成数据丢失。

<center><img src='https://pic1.zhimg.com/v2-36e739463bf4ec84e640476f256360da_720w.jpg?source=d16d100b'></center>

### 数据存取策略

#### 数据存放

HDFS 默认每个数据节点都在不同的机架上，这种方法会存在一个缺点，那就是**写入数据的时候不能充分利用同一机架内部机器之间的带宽**。但是，与这个缺点相比，这种方法也带来了更多很显著的优点

* 可以获得很高的数据可靠性，即使一个机架发生故障，位于其他机架上的数据副本仍然是可用的；
* 可以在多个机架上并行读取数据，大大提高数据读取速度；最后，可以更容易地实现系统内部负载均衡和错误处理

**HDFS 默认的冗余复制因子是 3**，每一个文件块会被同时保存到 3 个地方，其中，**有两个副本放在同一个机架的不同机器上面，第 3 个副本放在不同机架的机器上面，这样既可以保证机架发生异常时的数据恢复，也可以提高数据读写性能**

* 第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点
* 第二个副本：放置在与第一个副本不同的机架的节点上
* 第三个副本：与第一个副本相同机架的其他节点上
* 更多副本：随机节点

<center><img src='https://cdn.jsdelivr.net/gh/weno861/image/img/202401312139806.png'></center>

#### 数据读取

* HDFS提供了一个API可以确定一个数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID
* 当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据

#### 数据复制

当客户端向HDFS文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为3，当本地临时文件累积到一个数据块的大小时，客户端会从Namenode获取一个Datanode列表用于存放副本。然后客户端开始向第一个Datanode传输数据，第一个Datanode一小部分一小部分(4 KB)地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个Datanode节点。第二个Datanode也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个Datanode。最后，第三个Datanode接收数据并存储在本地。因此，Datanode能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个Datanode复制到下一个

## 数据错误与恢复

### 名称节点出错

Hadoop 采用两种机制来确保名称节点的安全

* 把名称节点上的元数据信息同步存储到其他文件系统，比如远程挂载的网络文件系统中
* 运行一个第二名称节点，当名称节点死机以后，可以把运行第二名称节点作为一种弥补措施，利用第二名称节点中的元数据信息进行系统恢复，但是这样做仍然会丢失部分数据

### 数据节点出错

每个数据节点会定期向名称节点发送「心跳」信息，向名称节点报告自己的状态。当数据节点发生故障，或者网络发生断网时，名称节点就无法收到来自一些数据节点的「心跳」信息，这时这些数据节点就会被标记为「死机」，节点上面的所有数据都会被标记为「不可读」，名称节点也不会再给它们发送任何 I/O 请求

### 数据出错

当客户端读取文件的时候，会先读取该信息文件，然后利用该信息文件对每个读取的数据块进行校验

